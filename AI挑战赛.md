# 参赛操作手册

## 达观杯文本智能挑战赛

### 1.挑战赛任务背景

-  这个比赛的任务就是文本分类，是nlp领域里面一个基本的任务，但这个比赛的难点在于，文本的长度特别长，大约3000个词，一般任务也就300词。
- 字：字组成词
- 词：一个词才有意义
- 中文分词：一句有字组成的句子经过中文分词工具，可以变成字-词-句子

### 2.用传统的监督学习模型对一段文本进行分类的基本过程：
- 一段原始文本  ->数据预处理
- 处理后的文本  ->特征工程(最重要步骤)
- Features     ->输入
- y=f(x1,x2,...)->输出
- 类别

### 3.比赛数据之训练数据集
- 脱敏数据：比赛中经常会把数据脱敏处理
- 提供模型用的

### 4.求解问题的本质
- 求一个数学函数（又可称为机器学习模型）：

      y= f（x1，x2，...,xn）
    使模型预测能力更强

### 5.比赛数据之测试数据集
- 检验模型好坏

### 6.机器学习算法
- 传统的监督学习算法：对数几率回归/支持向量书/朴素贝叶斯/决策树/集成学习等
- 深度学习：cnn/rnn/attention模型

### 7.如何提高模型性能
1. 数据预处理：缺失/错误数据
2. 特征工程：最重要
3. 机器学习算法
4. 模型集成

### 8.文本分类任务基本框架
![image](.\aicp1.png)
在很多比赛中，最重要的就是提取特征

#### 特征工程
- 文本特征提取
  - 经典的文本特征：前人研究的成熟的理论
    - TF
    - TFIDF
    - Doc2vec
    - Word2vec
  - 手工构造新特征：新创可能好的新特征
    - 寻找可能影响分类的新特征
    - 人工构造可能影响分类的新特征
  - 用神经网络提取：用神经网络的某一层输出作为样本的特征
   ![image](.\aicp2.png)

- 特征选择
  - 为什么要进行特征选择？
    - 减弱维数灾难，计算量降低
    - 降低学习任务难度
  - 特征选择的方法有哪些？(见西瓜书)
    - 嵌入式：
    - 包裹式：随机从所有特征中抽出中一部分特征。时间消耗较大
    - 过滤式：单独考虑某个特征的重要性
- 特征降维：将原始特征进行数学变换
  - 有监督降维：LDA(西瓜书第三章)
  - 无监督降维：
    - LSA
    - Ida
    - NMF

#### 分类器
   ![image](.\aicp3.png)
  - 一般数据挖掘中首选逻辑回归或lightgbm或xgboost  
  - 面对大规模的稀疏特征时，优先用逻辑回归  
  - lightgbm或xgboost：都是基于gbdb算法实现的，是梯度回归算法中的一种（统计学习方法8.4.3节）
  - 提分关键：多个单模型进行融合
    - 投票法：各个单模型进行统计，选择数量最多的那个。简单不一定不好。
    - 学习法：见西瓜书
    - 构造多个不同的训练集：多次随机采样
![image](.\aicp4.png)
